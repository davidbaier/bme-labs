{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubern-mia/bme-labs/blob/main/Session3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 3\n",
        "\n",
        "Today, we will look into what features a classifier deems important for the decision it takes.\n",
        "\n",
        "First, load the data againg (the same one you used last time)."
      ],
      "metadata": {
        "id": "Yoxiw15Jty5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiQgcnj7tuBa"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "! cd \"/content\"\n",
        "uploaded = files.upload()\n",
        "\n",
        "measurements = \"/content/\" + list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As last time, define a training and testing split and define the feature columns. Then we will use a random forest classifier for the prediction. Instead of the cross-validation as last time, we use the full training set."
      ],
      "metadata": {
        "id": "K_RiIDqJKVlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
        "\n",
        "# define a mapping of the disease encoding back to strings\n",
        "diseasestatus = {0: \"Healthy\", 1: \"ASD\", 2: \"Epilepsy\"}\n",
        "\n",
        "measurements = pd.read_csv(\"/content/fsfaststats.csv\")\n",
        "\n",
        "# map the disease name to the encoding and make sure the age is a float\n",
        "measurements[\"Disease\"] = [diseasestatus[e] for e in measurements[\"Disease\"]]\n",
        "measurements[\"Age\"] = [float(e) for e in measurements[\"Age\"]]\n",
        "\n",
        "# X is the feature matrix we feed to the classifier, i.e. the measurements and the age of the subject\n",
        "features = list(set(measurements.columns) - set([\"Subject\", \"Disease\"]))\n",
        "X = measurements.loc[:, features]\n",
        "y = measurements[\"Disease\"]\n",
        "\n",
        "# Reserve 20% of the data for testing\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  print(\"Training set size: \" + str(len(train_index)))\n",
        "  print(\"Test set size: \" + str(len(test_index)))\n",
        "\n",
        "X_train = X.iloc[train_index, :]\n",
        "y_train = y.iloc[train_index]\n",
        "\n",
        "X_test = X.iloc[test_index, :]\n",
        "y_test = y.iloc[test_index]\n",
        "\n",
        "# Plug in the parameter settings that work well according to your experiments\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
        "\n",
        "# Train the classifier and apply it to the test set\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "pVg3ai5GKCqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this trained classifier, we can now get information on which features are the most important ones:"
      ],
      "metadata": {
        "id": "cimTY0ZvLDfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featimportances = clf.feature_importances_\n",
        "\n",
        "# sort these in descending order and see which features they correspond to\n",
        "sortidx = (-featimportances).argsort()\n",
        "importances_sorted = featimportances[sortidx]\n",
        "featurenames_sorted = X_train.columns[sortidx]\n",
        "print(importances_sorted)\n",
        "print(featurenames_sorted)"
      ],
      "metadata": {
        "id": "O3FM2v87LU-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the feature importances. Since they are so many, limit it to the top 15 features."
      ],
      "metadata": {
        "id": "2O7aUZeeNu2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "n_most_important = 20\n",
        "sns.barplot(x=importances_sorted[:n_most_important], \n",
        "            y=featurenames_sorted[:n_most_important], color='forestgreen')\n",
        "plt.title(\"Random forest feature importance\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIPCC1b5NuHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you know roughly which features seem to be the most important ones. Now try to see if you can already see the importance by looking at them individually. For this, you can (as we did during the previous sessions), plot the features. Replace the example below where the left hippocampus is selected with the most important feature. Please do this for the top three according to your previous findings."
      ],
      "metadata": {
        "id": "4Y4RLk6EOx2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotcols = list(set(measurements.columns) - set([\"Age\", \"Subject\", \"Disease\"]))\n",
        "feature = \"Left-Hippocampus\"\n",
        "\n",
        "sns.lmplot(x=\"Age\", y=feature, hue=\"Disease\", data=measurements, ci=95)\n",
        "plt.title(feature)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P5eHzC4-uN0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permutation-based feature importances\n",
        "We also can gain insight into the importance of features for classifiers that do not offer a built-in technique such as you have seen for the Random Forest classifier. Additionally, the impurity-based feature importance you have used before can lead to misleading results if you have many unique feature values.\n",
        "Permutation-based feature importance can also give you a better impression of the behaviour on the test set, since the previous method is derived from model parameters inferred from the training set.\n",
        "\n",
        "The code below shows an example on how to get these feature imporances for the Random Forest classifier you have used before, but it can be replaced by any other classifier you tested.\n",
        "\n",
        "For this example, we will use the balanced accuracy as a performance metric.\n"
      ],
      "metadata": {
        "id": "84-GMYzAKuDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(clf, X_test, y_test, n_repeats=10, \n",
        "                                random_state=42, n_jobs=5, \n",
        "                                scoring='balanced_accuracy')\n",
        "\n",
        "# put the feature importances into a dataframe with the mean and standard \n",
        "# deviation across the repeated runs\n",
        "forest_importances = pd.DataFrame(zip(result.importances_mean, \n",
        "                                      result.importances_std), \n",
        "                                  index=X_test.columns, \n",
        "                                  columns=[\"Importance mean\", \"Importance std\"])"
      ],
      "metadata": {
        "id": "w4XHOKRmRTJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's sort that according to the mean importance column\n",
        "forest_importances_sorted = forest_importances.sort_values(\n",
        "    by=['Importance mean'], ascending=False)\n",
        "\n",
        "# to not clutter the plot too much, limit ourselves to the n_most_important \n",
        "# features as before\n",
        "plotting_subset = forest_importances_sorted.iloc[0:n_most_important, :]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plotting_subset[\"Importance mean\"].plot.bar(\n",
        "    yerr=plotting_subset['Importance std'], ax=ax, color='forestgreen')\n",
        "ax.set_title(\"Permutation feature importances\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E6eN6PK1TV9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we use the age of the patient as a feature and do not explicitly model the aging behaviour, plotting the distributions of the feature by healthy/diseases separately may also be informative (opposed to the feature values vs. age as you did before). To get a clearer plot, the following example only plots the ten most important features."
      ],
      "metadata": {
        "id": "l8ErZITtfXVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_top_features = 10\n",
        "measurements[\"Disease\"]\n",
        "subset = list(plotting_subset.index[:n_top_features]) + ['Disease']\n",
        "important_measurements = measurements[subset]\n",
        "\n",
        "# convert the dataframe to long format\n",
        "imp_meas_long = pd.melt(important_measurements, id_vars='Disease',\n",
        "        var_name='Feature', value_name='Feature value')"
      ],
      "metadata": {
        "id": "3M3JPftjZYah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 5))\n",
        "sns.boxplot(x='Feature', y='Feature value', hue=\"Disease\", data=imp_meas_long, \n",
        "            palette=\"Set2\", ax=ax)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
        "plt.title(\"Distribution by disease for important features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rxc3kChacUxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}